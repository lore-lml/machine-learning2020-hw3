{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "hw3-Domain Adaptation.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "NFy8f2ZOb83q",
        "colab_type": "text"
      },
      "source": [
        "**Install Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_6xQu_Fp1S1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install 'torch==1.4.0'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "7byL80PWb83r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "if not os.path.isdir('./Pacs'):\n",
        "  !git clone https://github.com/lore-lml/machine-learning2020-hw3.git\n",
        "  !mv 'machine-learning2020-hw3' 'Pacs'\n",
        "  !rm './Pacs/hw3.ipynb'\n",
        "  !rm './Pacs/README.md'\n",
        "\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from Pacs.pacs_dataset import Pacs\n",
        "from Pacs.dann import alexdann\n",
        "from Pacs.dann import train_src, test_target, dann_train_src_target\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "9Fmn_TeAb83t",
        "colab_type": "text"
      },
      "source": [
        "**Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "BKmhLZ0xb83t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "NUM_CLASSES = 7\n",
        "\n",
        "BATCH_SIZE = 128     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 1e-2        # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 10      # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 7       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "ALPHA = 0.1\n",
        "BASE_FILE_PATH = \"DA_RUN18_LR3e-5_ADAMW_EP15_SS15_G01_ALL_TRANSF\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "6sa6Y8Pzb83v",
        "colab_type": "text"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "A5W7Cni-b83w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transforms = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # Normalizes tensor with mean and standard deviation\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "P6EG9H2ob83y",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "D39D-n0Qb83y",
        "colab_type": "code",
        "outputId": "cdbcbd48-4e93-407b-9d9b-13decc615b6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "ROOT = 'Pacs/PACS'\n",
        "\n",
        "source_data = Pacs(ROOT, transform=transforms, source='photo')\n",
        "target_data = Pacs(ROOT, transform=transforms, source='art_painting')\n",
        "\n",
        "_, source_labels = source_data.get_img_with_labels()\n",
        "_, target_labels = target_data.get_img_with_labels()\n",
        "\n",
        "print(f\"# classes source_data: {len(set(source_labels))}\")\n",
        "print(f\"# classes val_set: {len(set(target_labels))}\")\n",
        "print(f\"source_data: {len(source_data)} elements\")\n",
        "print(f\"target_data: {len(target_data)} elements\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# classes source_data: 7\n",
            "# classes val_set: 7\n",
            "source_data: 1670 elements\n",
            "target_data: 2048 elements\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "BK7TcuLPb830",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "ou9I6xRrb831",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source_dataloader = DataLoader(source_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "target_dataloader = DataLoader(target_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_dataloader = DataLoader(target_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "9-Olxfg0b833",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "GJ3NssxCb833",
        "colab_type": "code",
        "outputId": "de517d2b-4f44-4168-db36-2fcfde75cf85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def init_cnn_objects(model):\n",
        "  \n",
        "  # Define loss function\n",
        "  criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "  parameters_to_optimize = model.parameters() # In this case we optimize over all the parameters of AlexNet\n",
        "  \n",
        "  optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "  #optimizer = optim.Adam(parameters_to_optimize, lr=LR,amsgrad=True)\n",
        "  #optimizer = optim.AdamW(parameters_to_optimize, lr=LR,amsgrad=True, weight_decay=WEIGHT_DECAY)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "  return criterion, optimizer, scheduler\n",
        "\n",
        "dann = alexdann(pretrained=True)\n",
        "criterion, optimizer, scheduler = init_cnn_objects(dann)\n",
        "print(\"******* NET CREATED *******\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******* NET CREATED *******\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "s0JrvlXOb835",
        "colab_type": "text"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "VnMriGBgb836",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_train_test(model, source_dataloader, test_dataloader, file_path=BASE_FILE_PATH):\n",
        "    train_losses = []\n",
        "    loss_min = -1\n",
        "    \n",
        "    model = model.to(DEVICE)\n",
        "    cudnn.benchmark\n",
        "    \n",
        "    current_step = 0\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "        cumulative_loss, current_step = train_src(model, source_dataloader, optimizer, criterion, current_step, DEVICE)\n",
        "        curr_loss = cumulative_loss / len(source_dataloader)\n",
        "        train_losses.append(curr_loss)\n",
        "        if loss_min == -1 or loss_min > curr_loss:\n",
        "            loss_min = curr_loss\n",
        "            torch.save(model, f\"{file_path}_best_model.pth\")\n",
        "        scheduler.step()\n",
        "        \n",
        "    model = torch.load(f\"{file_path}_best_model.pth\").to(DEVICE)\n",
        "    accuracy = test_target(model, test_dataloader, criterion, DEVICE) / float(len(target_data))\n",
        "    print(f\"Accuracy on test set: {accuracy}%\")\n",
        "    return train_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpmSLmS5eglj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "9e534999-9acc-4e27-aa68-3841be560a54"
      },
      "source": [
        "train_losses = simple_train_test(dann, source_dataloader, test_dataloader)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/10, LR = [0.01]\n",
            "Step 0, Loss_train 2.0163979530334473\n",
            "Step 10, Loss_train 0.14839710295200348\n",
            "Starting epoch 2/10, LR = [0.01]\n",
            "Step 20, Loss_train 0.29159054160118103\n",
            "Starting epoch 3/10, LR = [0.01]\n",
            "Step 30, Loss_train 0.1257377415895462\n",
            "Starting epoch 4/10, LR = [0.01]\n",
            "Step 40, Loss_train 0.02048277109861374\n",
            "Step 50, Loss_train 0.07205040007829666\n",
            "Starting epoch 5/10, LR = [0.01]\n",
            "Step 60, Loss_train 0.027401000261306763\n",
            "Starting epoch 6/10, LR = [0.01]\n",
            "Step 70, Loss_train 0.004358857870101929\n",
            "Starting epoch 7/10, LR = [0.01]\n",
            "Step 80, Loss_train 0.016971617937088013\n",
            "Step 90, Loss_train 0.006642531603574753\n",
            "Starting epoch 8/10, LR = [0.0001]\n",
            "Step 100, Loss_train 0.0011034831404685974\n",
            "Starting epoch 9/10, LR = [0.001]\n",
            "Step 110, Loss_train 0.0011677518486976624\n",
            "Starting epoch 10/10, LR = [0.001]\n",
            "Step 120, Loss_train 0.00018543750047683716\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:06<00:00,  2.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test set: 0.48486328125%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXNY7Wnw7IQm",
        "colab_type": "text"
      },
      "source": [
        "**Training with DANN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCp3Tzfw7Kwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dann_train_test(model, source_dataloader, target_dataloader, test_dataloader, file_path=BASE_FILE_PATH):\n",
        "    src_losses_y = []\n",
        "    src_losses_d = []\n",
        "    tgt_losses_d = []\n",
        "    loss_min = -1\n",
        "\n",
        "    len_dataloader = min(len(source_dataloader), len(target_dataloader))\n",
        "    \n",
        "    model = model.to(DEVICE)\n",
        "    cudnn.benchmark\n",
        "    \n",
        "    current_step = 0\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "        cum_loss_src_y, cum_loss_src_d, cum_loss_tgt_d, current_step = dann_train_src_target(model, source_dataloader, target_dataloader, optimizer, criterion, current_step, alpha=ALPHA, device=DEVICE)\n",
        "        curr_loss = cum_loss_src_y / len_dataloader\n",
        "        src_losses_y.append(curr_loss)\n",
        "        src_losses_d.append(cum_loss_src_d / len_dataloader)\n",
        "        tgt_losses_d.append(cum_loss_tgt_d / len_dataloader)\n",
        "        if loss_min == -1 or loss_min > curr_loss:\n",
        "            loss_min = curr_loss\n",
        "            torch.save(model, f\"{file_path}_best_model.pth\")\n",
        "        scheduler.step()\n",
        "        \n",
        "    model = torch.load(f\"{file_path}_best_model_dann.pth\").to(DEVICE)\n",
        "    accuracy = test_target(model, test_dataloader, criterion, DEVICE) / float(len(target_data))\n",
        "    print(f\"Accuracy on test set: {accuracy}%\")\n",
        "    return src_losses_y, src_losses_d, tgt_losses_d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bibb5WjFqaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dann = alexdann(pretrained=True)\n",
        "criterion, optimizer, scheduler = init_cnn_objects(dann)\n",
        "src_losses_y, src_losses_d, tgt_losses_d = dann_train_test(dann, source_dataloader, target_dataloader, test_dataloader)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}